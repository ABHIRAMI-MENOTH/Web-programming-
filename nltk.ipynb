{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b367a-7ac1-4a7b-abf8-a9c8f355286d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bead343a-8256-45e0-8801-d07d4120bb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\abhir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abhir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\abhir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\abhir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\abhir\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: \n",
      " A newspaper is the strongest medium for news. People are \n",
      "reading newspapers for decades.  \n",
      "It has a huge contribution to globalization. Right now because of \n",
      "easy internet connection,  \n",
      "people don't read printed newspapers often. They read the online \n",
      "version. \n",
      "\n",
      "Tokenizing by sentence: \n",
      " ['A newspaper is the strongest medium for news.', 'People are \\nreading newspapers for decades.', 'It has a huge contribution to globalization.', \"Right now because of \\neasy internet connection,  \\npeople don't read printed newspapers often.\", 'They read the online \\nversion.'] \n",
      "\n",
      "Tokenizing by word: \n",
      " ['A', 'newspaper', 'is', 'the', 'strongest', 'medium', 'for', 'news', '.', 'People', 'are', 'reading', 'newspapers', 'for', 'decades', '.', 'It', 'has', 'a', 'huge', 'contribution', 'to', 'globalization', '.', 'Right', 'now', 'because', 'of', 'easy', 'internet', 'connection', ',', 'people', 'do', \"n't\", 'read', 'printed', 'newspapers', 'often', '.', 'They', 'read', 'the', 'online', 'version', '.'] \n",
      "\n",
      "After filtering the stop words and punctuation: \n",
      "newspaper\n",
      "strongest\n",
      "medium\n",
      "news\n",
      "People\n",
      "reading\n",
      "newspapers\n",
      "decades\n",
      "huge\n",
      "contribution\n",
      "globalization\n",
      "Right\n",
      "easy\n",
      "internet\n",
      "connection\n",
      "people\n",
      "n't\n",
      "read\n",
      "printed\n",
      "newspapers\n",
      "often\n",
      "read\n",
      "online\n",
      "version\n",
      "\n",
      "Given words:  ['reading', 'globalization', 'Being', 'Went', 'gone', 'going']\n",
      "After stemming:  ['read', 'global', 'be', 'went', 'gone', 'go'] \n",
      "\n",
      "rocks: rock\n",
      "corpora: corpus\n",
      "better: better\n",
      "believes: belief \n",
      "\n",
      "went as adjective: went\n",
      "went as verb: go\n",
      "went as noun: went \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging: \n",
      "\n",
      "('A', 'DT')\n",
      "('newspaper', 'NN')\n",
      "('is', 'VBZ')\n",
      "('the', 'DT')\n",
      "('strongest', 'JJS')\n",
      "('medium', 'NN')\n",
      "('for', 'IN')\n",
      "('news', 'NN')\n",
      "('.', '.')\n",
      "('People', 'NNS')\n",
      "('are', 'VBP')\n",
      "('reading', 'VBG')\n",
      "('newspapers', 'NNS')\n",
      "('for', 'IN')\n",
      "('decades', 'NNS')\n",
      "('.', '.')\n",
      "('It', 'PRP')\n",
      "('has', 'VBZ')\n",
      "('a', 'DT')\n",
      "('huge', 'JJ')\n",
      "('contribution', 'NN')\n",
      "('to', 'TO')\n",
      "('globalization', 'NN')\n",
      "('.', '.')\n",
      "('Right', 'RB')\n",
      "('now', 'RB')\n",
      "('because', 'IN')\n",
      "('of', 'IN')\n",
      "('easy', 'JJ')\n",
      "('internet', 'JJ')\n",
      "('connection', 'NN')\n",
      "(',', ',')\n",
      "('people', 'NNS')\n",
      "('do', 'VBP')\n",
      "(\"n't\", 'RB')\n",
      "('read', 'VB')\n",
      "('printed', 'JJ')\n",
      "('newspapers', 'NNS')\n",
      "('often', 'RB')\n",
      "('.', '.')\n",
      "('They', 'PRP')\n",
      "('read', 'VBD')\n",
      "('the', 'DT')\n",
      "('online', 'JJ')\n",
      "('version', 'NN')\n",
      "('.', '.')\n",
      "\n",
      "\n",
      "After Chunking:\n",
      " (S\n",
      "  (NP A/DT newspaper/NN)\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  strongest/JJS\n",
      "  (NP medium/NN)\n",
      "  for/IN\n",
      "  (NP news/NN)\n",
      "  ./.\n",
      "  People/NNS\n",
      "  are/VBP\n",
      "  reading/VBG\n",
      "  newspapers/NNS\n",
      "  for/IN\n",
      "  decades/NNS\n",
      "  ./.\n",
      "  It/PRP\n",
      "  has/VBZ\n",
      "  (NP a/DT huge/JJ contribution/NN)\n",
      "  to/TO\n",
      "  (NP globalization/NN)\n",
      "  ./.\n",
      "  Right/RB\n",
      "  now/RB\n",
      "  because/IN\n",
      "  of/IN\n",
      "  (NP easy/JJ internet/JJ connection/NN)\n",
      "  ,/,\n",
      "  people/NNS\n",
      "  do/VBP\n",
      "  n't/RB\n",
      "  read/VB\n",
      "  printed/JJ\n",
      "  newspapers/NNS\n",
      "  often/RB\n",
      "  ./.\n",
      "  They/PRP\n",
      "  read/VBD\n",
      "  (NP the/DT online/JJ version/NN)\n",
      "  ./.)\n",
      "                                                                                                                                                                                       S                                                                                                                                                                                                                                   \n",
      "   ____________________________________________________________________________________________________________________________________________________________________________________|___________________________________________________________________________________________________________________________________________________________________________________________________________________                 \n",
      "  |      |          |         |     |      |         |         |            |          |         |       |    |       |      |    |     |       |        |        |    |      |        |      |       |        |            |           |      |     |        |      |        NP                  NP       NP           NP                          NP                     NP                              NP              \n",
      "  |      |          |         |     |      |         |         |            |          |         |       |    |       |      |    |     |       |        |        |    |      |        |      |       |        |            |           |      |     |        |      |    ____|_______            |        |      ______|___________                |             _________|____________           ________|_________       \n",
      "is/VBZ the/DT strongest/JJS for/IN ./. People/NNS are/VBP reading/VBG newspapers/NNS for/IN decades/NNS ./. It/PRP has/VBZ to/TO ./. Right/RB now/RB because/IN of/IN ,/, people/NNS do/VBP n't/RB read/VB printed/JJ newspapers/NNS often/RB ./. They/PRP read/VBD ./. A/DT     newspaper/NN medium/NN news/NN a/DT huge/JJ contribution/NN globalization/NN easy/JJ internet/JJ connection/NN the/DT online/JJ version/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')   \n",
    "nltk.download('stopwords')  \n",
    "nltk.download('wordnet')   \n",
    "nltk.download('omw-1.4')   \n",
    "nltk.download('averaged_perceptron_tagger')   \n",
    "nltk.download('maxent_ne_chunker')   \n",
    " \n",
    "nltk.download('punkt_tab')   \n",
    " \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from string import punctuation \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer \n",
    "from nltk import RegexpParser \n",
    " \n",
    "# Sample text \n",
    "text = \"\"\"A newspaper is the strongest medium for news. People are \n",
    "reading newspapers for decades.  \n",
    "It has a huge contribution to globalization. Right now because of \n",
    "easy internet connection,  \n",
    "people don't read printed newspapers often. They read the online \n",
    "version.\"\"\" \n",
    "print(\"Sample text: \\n\", text, \"\\n\") \n",
    " \n",
    "# Tokenizing by sentence \n",
    "sent_tokenized = sent_tokenize(text) \n",
    "70 \n",
    " \n",
    "print(\"Tokenizing by sentence: \\n\", sent_tokenized, \"\\n\") \n",
    " \n",
    "# Tokenizing by word \n",
    "word_tokenized = word_tokenize(text) \n",
    "print(\"Tokenizing by word: \\n\", word_tokenized, \"\\n\") \n",
    " \n",
    "# Removing stop words and punctuation \n",
    "stop_words = set(stopwords.words('english')) \n",
    "punctuation_set = set(punctuation) \n",
    " \n",
    "print(\"After filtering the stop words and punctuation: \") \n",
    "filtered_words = [word for word in word_tokenized if word.casefold() \n",
    "not in stop_words and word.casefold() not in punctuation_set] \n",
    "for word in filtered_words: \n",
    "    print(word) \n",
    " \n",
    "# Stemming \n",
    "ps = PorterStemmer() \n",
    "words = [\"reading\", \"globalization\", \"Being\", \"Went\", \"gone\", \n",
    "\"going\"] \n",
    "print(\"\\nGiven words: \", words) \n",
    "stemm = [ps.stem(i) for i in words] \n",
    "print(\"After stemming: \", stemm, \"\\n\") \n",
    " \n",
    "# Lemmatization \n",
    "lem = WordNetLemmatizer() \n",
    "print(\"rocks:\", lem.lemmatize(\"rocks\")) \n",
    "print(\"corpora:\", lem.lemmatize(\"corpora\")) \n",
    "print(\"better:\", lem.lemmatize(\"better\")) \n",
    "print(\"believes:\", lem.lemmatize(\"believes\"), \"\\n\") \n",
    " \n",
    "# Lemmatization with POS tag \n",
    "print(\"went as adjective:\", lem.lemmatize(\"went\", pos=\"a\")) \n",
    "print(\"went as verb:\", lem.lemmatize(\"went\", pos=\"v\")) \n",
    "print(\"went as noun:\", lem.lemmatize(\"went\", pos=\"n\"), \"\\n\") \n",
    " \n",
    "# POS tagging \n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "postag = nltk.pos_tag(word_tokenized) \n",
    "print(\"POS tagging: \\n\") \n",
    "for i in postag: \n",
    "    print(i) \n",
    "print(\"\\n\") \n",
    " \n",
    "# Chunking \n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n",
    "chunker = RegexpParser(grammar) \n",
    "output = chunker.parse(postag) \n",
    "print(\"After Chunking:\\n\", output) \n",
    "output.pretty_print() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24690617-d03f-4237-b3c3-41da1ca8e36a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
